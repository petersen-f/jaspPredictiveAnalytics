<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="jaspPredictiveAnalytics">
<title>Univariate Predictive Analytics - Simulated data set • jaspPredictiveAnalytics</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><link href="../deps/Roboto-0.4.2/font.css" rel="stylesheet">
<link href="../deps/Roboto_Slab-0.4.2/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Univariate Predictive Analytics - Simulated data set">
<meta property="og:description" content="jaspPredictiveAnalytics">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">jaspPredictiveAnalytics</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/getting_started.html">Get Started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/tutorials.html">Tutorials</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/background.html">Background</a>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://jasp-stats.org/">JASP</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/petersen-f/jaspPredictiveAnalytics" aria-label="Github">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Univariate Predictive Analytics - Simulated data set</h1>
                        <h4 data-toc-skip class="author">Fridtjof
Petersen</h4>
            
      
      
      <div class="d-none name"><code>quality_control.Rmd</code></div>
    </div>

    
    
<p>Welcome to the first tutorial of the Univariate Predictive Analytics
Module! The main functionality is to predict whether a process will go
out-of-control in the future. The module is quite extensive but we can
break down the process into 3 steps:</p>
<ol style="list-style-type: decimal">
<li>Determining the control bounds</li>
<li>Selecting and evaluating the candidate models</li>
<li>Predicting the future with the best model</li>
</ol>
<p>The basic idea is to first set the control limits, select various
possible models and test their predictive accuracy on past data and
finally use these results to select the final model that we want to use
to predict the future. By using past performance as a proxy for future
prediction performance we should be able to make a more informed choice
of which model will perform the best.</p>
<p>We will demonstrate the functionality on a simulated data set that is
supposed to represent a real world production process. Our dependent
variable is the size of our produced good that is measured at the end of
the production cycle. Since one production cycle for one piece takes
approximately 100 data points from start to finish, our goal is to
predict 100 data points into the future. That way we could intervene if
the results of the next cycle were predicted to go out of control.
Because if we would only notice at the end of the production cycle we
would need to discard all the pieces from the previous cycle. We also
have some measures from arbitrary sensors (could represent temperature,
vibration or machine part age) measured at the start of the cycle that
are somewhat related to the process. As we want to predict into the
future we use the sensors measurement at the beginning of the cycle - in
other words the sensor measurements for each piece are lagged by 100
data points.</p>
<p>The values of the process are primarily influenced by the heat of the
machinery which depends on whether the machine is turned on over night
or how long it is turned off for maintenance. It runs everyday from
09:00 until 17:00 and produces 60 pieces per minute. The current data
set represents 7 days of data and we want to predict the end.</p>
<div class="section level2">
<h2 id="determining-the-control-bounds">Determining the control bounds<a class="anchor" aria-label="anchor" href="#determining-the-control-bounds"></a>
</h2>
<div class="section level3">
<h3 id="selecting-necessary-variables">Selecting necessary variables<a class="anchor" aria-label="anchor" href="#selecting-necessary-variables"></a>
</h3>
<p>As a first step we need to load the data into JASP which is shown in
the following image:</p>
<div class="float">
<img src="img/univar_tutorial_sim/add_variables.gif" width="600" alt="Figure 1: Selecting necessary variables"><div class="figcaption">Figure 1: Selecting necessary variables</div>
</div>
<p>But what do of these different options mean? Our <strong>Dependent
Variable <em>y</em></strong> is the continuous production process we
want to monitor and predict. The <strong>Time</strong> variable
<strong><em>time</em></strong> contains the time stamps that must be
supplied. Supplying <strong>Covariates</strong> and
<strong>Factors</strong> is optional since they are not necessary for
the classical time series models - but are needed for more complex
models. In our case we include them as our sensor measurements are
associated with our dependent variable and could thus improve predictive
accuracy. Additionally, we have two variables called
<strong><em>day_new_time_since</em></strong> and
<strong><em>day_new_n_since</em></strong> that indicate how many pieces
have been produced since and how much time has passed since the start of
that day. They are useful because the time the machine has been on
directly relates to its temperature - which affects the size of our
produced pieces. Lastly, the <strong>Include in Training</strong>
variable indicates whether a time point should be used for training and
verifying the model (set to 1) or whether it should be used to predict
the future (set to 1). While the observations of the dependent variable
are of course not available for these data points as we want to predict
them, it is needed to supply covariates and factors if we included them
in our models.</p>
</div>
<div class="section level3">
<h3 id="control-plot-and-control-limits">Control plot and control limits<a class="anchor" aria-label="anchor" href="#control-plot-and-control-limits"></a>
</h3>
<p>After the variables have been supplied, we automatically get a basic
control plot in the result section of JASP:</p>
<div class="float">
<img src="img/univar_tutorial_sim/control_plot_no_adj.JPG" width="659" alt="Figure 2: Default control plot based on 2 standard deviations"><div class="figcaption">Figure 2: Default control plot based on 2
standard deviations</div>
</div>
<p>The red dashed lines indicate the control bounds that we don’t want
to cross. As a default option, the control bounds are set in such a way
that all the data that is more than 2 standard deviations away from the
mean is flagged as out-of-control.</p>
<p>From mere visual inspection alone we can already see two things:
There seems to be a cyclic pattern where the process abruptly jumps in
size and slowly becomes slower again. As mentioned previously, this
directly relates to the effect of the temperature: Since we measure the
process over several days, the machine cools down at night when turned
off which explains the sudden jump. A lower temperature causes the
produced pieces to be larger. However when the machine is turned on
continuously, the temperature slowly rises to a certain maximum
temperature where the size of each piece is somewhat stable.</p>
<p>As a second observation the process seems to go out-of-control at the
end of the time series quite rapidly. The hypothetical explanation is
that a part inside the machine and broke and subsequently heats it up
drastically which decreases the size of our products. Since it is
undesirable to let the process exceed the control bounds, we will test
in this tutorial whether the module can accurately predict whether and
at which time point the process goes out-of-control in the past! This is
called forecast verification as we test whether our model predictions
can properly predict events in the past to make sure we can trust our
models. Afterwards we will illustrate how we can use the JASP module to
predict the unseen future.</p>
<p>But let us first adjust the control limits. In production processes
we often know how large/small a produced object must be so be don’t have
to set the control bounds based on data and standard deviations. For our
example, we will increase the control limits somewhat as only the last
part of the time series clearly goes out-of-control. We can do this
under the <em>Time Series Descriptives</em> section:</p>
<div class="float">
<img src="img/univar_tutorial_sim/select_bounds.gif" width="455" alt="Figure 3: Manually selecting custom control bounds"><div class="figcaption">Figure 3: Manually selecting custom control
bounds</div>
</div>
<p>Under the <em>Error Bound Selection</em> option we can select
<em>Manual Bounds</em> and then manually input the upper/lower limit of
8.2/7.15 which changes the plot into the following:</p>
<div class="float">
<img src="img/univar_tutorial_sim/adjusted_control_plot.png" width="514" alt="Figure 4: Adjusted control plot based on manually set limits"><div class="figcaption">Figure 4: Adjusted control plot based on
manually set limits</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="selecting-and-evaluating-the-candidate-models">Selecting and evaluating the candidate models<a class="anchor" aria-label="anchor" href="#selecting-and-evaluating-the-candidate-models"></a>
</h2>
<div class="section level3">
<h3 id="choosing-evaluation-plan">Choosing Evaluation plan<a class="anchor" aria-label="anchor" href="#choosing-evaluation-plan"></a>
</h3>
<p>As mentioned previously, we want to choose the model that predicts
the future the best. One method for this is called <em>cross
validation</em> where the existing data is randomly split into
<strong>training</strong> and <strong>test</strong> data sets. The
training set is used to <em>train</em> the statistical model which then
make <em>predictions</em> about the test set that are compared to the
real observation. By comparing the predictions with the true
observations we can test how well each model predicts the unseen data
and generalises.</p>
<p>As the goal is to evaluate the predictive performance on unseen
future observation, normal <em>cross validation</em> that randomly
selects training and test data is not appropriate. Instead, the data is
split in such a way that the training data is always temporally before
the test data. In our example we want to train the models on some amount
of past data and then use the next 100 data points as test data as this
constitutes one production cycle. Afterwards we shift the training
window ahead by a certain amount of data points, we retrain our models
and then predict the next 100 data points. This is called <em>rolling
window cross validation</em> or <em>historical forecast
evaluation</em>.</p>
<p>The settings for this procedure can be found under the “Forecast
Evaluation” section in the JASP module:</p>
<div style="float:right; text-align:left; width:246px; Padding: 10px">
<p><img src="img/univar_tutorial_sim/forecast_eval_plan.JPG" width="246px"></p>
<p>
Figure 5: Creating an evaluation plan
</p>
</div>
<p>The <em>Training window</em> option refers to how many past data
points our model is trained on. Since we experience an underlying shift
during the last day of the process and it goes out of control, we will
only train the models on the past 75 data points. In practice it might
be unknown on how much data each model should be trained on and
different options should be considered. If the underlying data
generating process stays constant over time, the models are ideally
trained on all of the past - this can be achieved by checking the option
for <em>Cumulative Training.</em> If the process changes (quickly) over
time - as indicated by worse predictive performance a shorter training
window might be a better choice.</p>
<p>The <em>Prediction window</em> option determines how many data points
should be predicted into the future. This is ideally chosen in such a
way that it corresponds to the prediction horizon one is interested in
predicting (as different models might perform better in short and other
in long-term forecasts). In our example, this corresponds to a
prediction horizon of 100 as this represents one full cycle.</p>
<p>The next option called <em>Skip between training slices</em>
indicates how many data points are shifted ahead for each prediction
slice.</p>
<p>The <em>Maximum nr. of slices option</em> determines how many folds
are created. Increasing this number leads to less prediction slices or
folds that are computed but might increase computation time. Especially
if computational time is scarce since we want to make a timely
prediction about the future, we might want to select a smaller number.
In our case we selected 6 as we want to only predict the last part of
the process where the data goes out-of-control.</p>
<p>When we check the <em>Show evaluation</em> option we can also see the
evaluation plan visualized to check which data the model will evaluate
(see Figure 6). Here we see that the blue colored data is the training
period that the models are trained on- whereas the red period will be
predicted and compared with the actual observations.</p>
<div class="float">
<img src="img/univar_tutorial_sim/evaluation_plan.png" width="600" alt="Figure 6: Plotted forecast evaluation plan"><div class="figcaption">Figure 6: Plotted forecast evaluation plan</div>
</div>
</div>
<div class="section level3">
<h3 id="selecting-candidate-models">Selecting candidate models<a class="anchor" aria-label="anchor" href="#selecting-candidate-models"></a>
</h3>
<p>Now that we have a plan to evaluate the data, we can select the
models that we want to use to predict the future. We can find the option
for selecting different models under the <strong>Forecast
Evaluation</strong> section under <strong>Model Choice</strong>. On the
left side of the selection option, all available models are displayed
while the right side indicates the models we have chosen for
prediction.</p>
<p>A little explanation on the naming convention for all models: The
model family is indicated as the first part of the model name before the
hyphen “-” and the text afterwards indicates the model specification.
For more a more in-depth explanation of all models and further resources
on them see the <a href="background.html#models">Background</a>
section.</p>
<div class="float">
<img src="img/univar_tutorial_sim/model_choices.JPG" width="455" alt="Figure 7: Displays all available prediction models"><div class="figcaption">Figure 7: Displays all available prediction
models</div>
</div>
<p>For example in our selected models, the <strong>prophet</strong>
model is a Bayesian time series model that automatically models
appropriate seasonality via fourier orders, trends and change points. As
it is a time series model, it predicts future observation of our
dependent variable based on its own past values. If we would want to add
covariates, we would select the <strong>prohpet - regression</strong>
model. Our second model denoted as <strong>bart - regression</strong> is
a Bayesian Additive Regression Tree model. It combines multiple decision
trees into a large ensemble model and is able to model high-dimensional
non-linear relationships. As it is not a time series by default, we need
to supply it at least some covariates which in our case are the sensors.
Additionally, it is also possible to supply it with our lagged dependent
variable. If we enable this option in the <strong>Feature
Engineering</strong> section, internally a new column is created which
contains our dependent variable but at a previous time point. That way
even models that are not time series models can model the relationship
of a variable with itself over time. When the model then predicts future
data points in our test set, the predicted value for time point
<strong>t</strong> is then iteratively used as a input covariate for the
prediction of time point <strong>t+1.</strong> Note that this takes a
long time for the BART model as the model is computationally
complex.</p>
<p>After we have selected which models we would like to use for
prediction, the models are being trained and evaluated based on the
evaluation plan we specified earlier. After the computations are
finished, we get a table with the corresponding evaluation metrics and
are also able to plot our predictions.</p>
</div>
<div class="section level3">
<h3 id="choosing-an-appropriate-evaluation-metrics">Choosing an appropriate evaluation metrics<a class="anchor" aria-label="anchor" href="#choosing-an-appropriate-evaluation-metrics"></a>
</h3>
<p>One can evaluate the prediction accuracy of our models by computing
so called evaluation or forecast metrics that measure how well our
models predict the future by comparing the actual observations of the
test set with the predictions. The JASP module offers both probabilistic
and point-based prediction metrics: Instead of relying on the point or
mean prediction of a model, probabilistic forecast metrics compute how
much the observed value differs from the whole predictive distribution.
That way we are able to take into account the uncertainty that is
associated with our predictive distribution. Ideally, our distribution
is very sharp as this indicates that we are very certain in our
prediction.</p>
<p>For the current example we focus on the mean absolute error (MAE),
the Continuous Ranked Probability Score (CRPS) and the R-squared value.
As the name suggests the MAE is calculated by calculating how each
observations differs from the forecast in absolute terms (i.e. removing
the minus of negative numbers) and averaging it across all predicted
observations. The CRPS on the other hand generalizes the MAE as it
computes it for the whole predictive distribution instead of only a
single forecast value for each observation. The R-squared value tells us
how much variance is explained by the forecast. The other forecast
metrics are explained in detail <a href="general_documentation.html#forecast-evaluation-metric-table">here</a>.
In general, all forecast evaluation metrics can be selected under the
section <strong>Forecast Evaluation -&gt; Evaluation
Metrics</strong>:</p>
<p><img src="img/univar_tutorial_sim/evaluation_metrics.JPG" width="450"></p>
<div class="section level4">
<h4 id="forecast-evaluation-metric-table">Forecast Evaluation Metric Table<a class="anchor" aria-label="anchor" href="#forecast-evaluation-metric-table"></a>
</h4>
<p>Now let’s look at the table with forecast evaluation metrics. The
first row indicates the corresponding model name. The other rows
indicate the values of the corresponding evaluation metric. The values
are averaged across all evaluation slices set previously. Lower MAE and
CRPS values indicate that our prediction performs better while higher
values of <span class="math inline">\(R^2\)</span> indicate a better
prediction. While MAE and CRPS are dependent on the scale of our data
(i.e. if I were to multiply all observations by 10, the MAE values would
also increase by 10), the <span class="math inline">\(R^2\)</span>
values are scale independent where a value of <span class="math inline">\(R^2 = 1\)</span> would indicate that the
observations are predicted perfectly by our model and a value of <span class="math inline">\(R^2 =0\)</span> that our predictions fail
completely. By comparing the values for each model we can determine
which of our candidate models predicts the best.</p>
<p>From comparing the prophet and BART model with each other, we can see
that the prediction of the prophet model generally predict better as
both the CRPS and MAE values are lower. On the other hand the <span class="math inline">\(R^2\)</span> value for the BART model is much
higher which can be explained by the fact that the model is more
flexible. Thus it explains more variance of the observed data, while
this extra flexibility makes the predictions more wrong on average - as
measured by the MAE and CRPS. This will become more obvious if we
observe the plots of the data.</p>
<p><img src="img/univar_tutorial_sim/metric_table.JPG" width="266"></p>
</div>
<div class="section level4">
<h4 id="forecast-plots">Forecast Plots<a class="anchor" aria-label="anchor" href="#forecast-plots"></a>
</h4>
<p>We can also visualize the predictions of each model against the real
observations. We can select the models we want to view under the section
<strong>Prediction Plots.</strong></p>
<p><img src="img/univar_tutorial_sim/model_prediction_plot.JPG" width="450"></p>
<p>Afterwards the corresponding prediction plot is shown in the JASP
output. As shown in the legend at the bottom, the different line colors
represent the different model types. The real observed data is shown in
purple, the prophet model in green and the BART model in yellow. The
real data depicts both the training window used to trail all models as
well as the prediction horizon that was used to calculate the evaluation
metrics. The two dashed red lines depict the control limits so we can
verify whether the models accurately predict that the data goes out of
control. Each evaluation slice is shown as a separate plot.</p>
<p><img src="img/univar_tutorial_sim/prediction_plot.png" width="643"></p>
<p>In general, we can see that the data is predicted quite well by both
models. The process only seems to go really out of control in the last
two slices/around data point 2500 which is more visible in the overall
control plot in Figure 4 at the beginning of the tutorial. The prophet
model seems to be better at predicting the overall trend of the data,
while the BART model predicts the fluctuation of the data better and
does not pick up as well on the downward trend. This makes sense as the
prophet model is designed to predict the trend and seasonality of the
time series data, while the BART model is better at predicting
relationships with the covariates and does not explicitly model the
relationship of the process with itself. This also aligns with the
results from the evaluation metric table that was shown in the previous
step: The prophet model also had the lower error as measured by the MAE
and CRPS while the BART model explained more variance of the real
observations. In general we can conclude that we do a decent job at
predicting that the process goes out of control.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="selecting-the-best-model-for-future-prediction">Selecting the best model for future prediction<a class="anchor" aria-label="anchor" href="#selecting-the-best-model-for-future-prediction"></a>
</h2>
<p>After we have evaluated the predictive performance of our models on
on past data, we are ready to select the model from our candidate pool
that we want to use to predict the future. From the metric table and
prediction plot, we have observed that the prophet model does a better
job at predicting the overall trend of the process, while the BART model
is better at predicting the variance in the data. In an ideal world, we
would have a single model that is good at both trend forecasting and
complex covariate relationships. When this is not feasible from a
computational or statistical standpoint however, we can instead combine
the predictions from different models. This way one doesn’t have to
choose manually between different models - which might benefit applied
user of this module - and one can potentially improve predictive
accuracy compared to choosing any single model. One method to combine
different model prediction - and which is implemented in this module -
is called forecasts <strong>ensemble Bayesian model averaging</strong>.
It is discussed briefly <a href="getting_started.html#ensemble-bayesian-model-averaging">here</a>
and in more extensively <a href="background.html#ensemble-bayesian-model-averaging">here</a>.</p>
<div class="section level3">
<h3 id="ensemble-bayesian-model-averaging">Ensemble Bayesian Model Averaging<a class="anchor" aria-label="anchor" href="#ensemble-bayesian-model-averaging"></a>
</h3>
<p>In general, ensemble Bayesian model averaging (eBMA) consists of two
steps: First, model weights are assigned to all models based on their
past predictive performance. The weights are also called posterior model
probabilities as it can be interpreted as the relative likelihood that
the true observations originated from the corresponding evaluated model
- compared to all other considered models. Secondly, these weights are
then used to weight the individual model predictions for future data and
finally sum them to combine them into a single ensemble model. In the
present JASP model, the weights are estimated separately for each
evaluation slice, and then used to adjust the predictions for the next
slice. Then we compute the evaluation metrics for the eBMA predictions
so we can determine the predictive benefit compared to any single model.
Finally, we use the model weights from the last evaluation slice to
adjust the out-of-sample predictions for the future.</p>
<p>In order to use ensemble Bayesian model averaging (eBMA) in JASP, we
simply need to check the checkbox <em>Perform eBMA</em> in the section
Ensemble <strong>Bayesian Model Averaging</strong>. Prerequisite for
this is that we have performed forecasting evaluation with at least two
models previously. The <em>Method</em> option determines how the model
weights are determined and are by default set to the faster
Expectation-maximization algorithm. The <em>Evaluation Method</em>
determines how the predictive benefit of eBMA is evaluated.
Additionally, we can also show the model weights averaged across all
evaluation slices - or per individual slice if we want to know
specifically what weights were used to adjust the prediction</p>
<p><img src="img/univar_tutorial_sim/perform_bma.JPG" width="450"></p>
<p>As a result we obtain a table with the model weights and an adjusted
evaluation metric table that includes the eBMA model. Across all slices
the prophet model receives a slightly higher model weight which
indicates that it is more likely to be the best model. This is in line
with the evaluation table where the prophet model outperformed the BART
model only slightly as well in terms of MAE and CRPS.</p>
<p>When looking at the evaluation metrics we can see that the eBMA
predictions decreases the CRPS values by ~17 percent and the the MAE
values ~15 percent compared to the prophet model. When looking at the
<span class="math inline">\(R^2\)</span> values, the eBMA approach
increased the metric by 12.5 percent. So based on these metrics we can
argue that combining the predictions of different model via eBMA has
indeed improved prediction accuracy substantially.</p>
<!--# maybe add an explanation that these metrics do NOT measure how often the module correctly predicts - For that we would need more data to properly quantify typical classification metrics -->
<div>
<p><img src="img/univar_tutorial_sim/bma_weights.JPG" style="float: left" width="178"></p>
<p><img src="img/univar_tutorial_sim/metric_table_bma.JPG" width="318"></p>
</div>
<p>But how do 15- 20 percent actually look like? In our example data set
we hypothetically want to predict whether a process goes out of control.
Is the increase in prediction enough to make better decisions of whether
we should turn the machine off or intervene in a different way ahead of
time? For this we can look at the prediction plots we discussed earlier.
We can now also visualize the eBMA predictions. As an example we will
only investigate two prediction slices. In both cases the predictions of
the prophet model (blue) are a bit too low and the predictions of the
BART model are a bit too high (green). The eBMA predictions however are
better overall and somewhat closer to the real observations displayed in
purple.</p>
<p><img src="img/univar_tutorial_sim/bma_prediction.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="predicting-the-future">Predicting the Future<a class="anchor" aria-label="anchor" href="#predicting-the-future"></a>
</h3>
<p>So far we have set the control limits, evaluated all models and
explored whether the eBMA approach can help us in making better
predictions. Now it is finally time to predict the actual future. Let us
pretend that a new day has started and tested whether the module could
in principle predict whether the process goes out-of-control. At the
same time we fixed our production machine and replaced some parts to
hopefully stop the machine from going out of control again. Let us also
pretend the database connected to our module has been updated and new
data has already arrived. The process has not gone out of control and
the corresponding control plot looks like this:</p>
<p><img src="img/univar_tutorial_sim/control_plot_future.png" width="600"></p>
<p>To predict the future, we need to open the section titled
<strong>Future Prediction</strong>. The first option lets us select the
model that we want to use to predict the future. Here we simply use the
ensemble Bayesian Model Averaging as it weights all selected models
automatically for us - and as previously shown can provide a predictive
benefit over a single model. The model weights are taken from the most
recent training to adjust the future predictions.</p>
<p>The values for the prediction horizon (how far we want to predict
into the future) - as well as the training window (how much of past data
we want to use for training) are by default set to the same values that
we selected for the model evaluation. Alternatively, we could also set
custom values or train the models on all of the past models. The
<em>Future prediction plot</em> checkbox is automatically enabled as
soon as a model is chosen for prediction.</p>
<p><img src="img/univar_tutorial_sim/predict_future.JPG" width="600"></p>
<p>Now lastly we can look at the prediction plot that appears as soon as
the computations have finished. The horizontal red dashed lines again
represent the control bounds. The data depicted in black before the
vertical dashed line represent the real data while the data afterwards
depicts the predicted future. The blue area depicts the 95 percent
<em>credible interval</em> and can be interpreted in the following way:
Given all past data and the predictions of the different models, the
eBMA method predicts that the unknown future observations will fall
within this interval with a probability of 95 percent. As neither the
black line (mean prediction) nor the credible interval fall outside of
the control limits, we can assume that the process will not go
out-of-control within the next 100 data points.</p>
<p><img src="img/univar_tutorial_sim/future_prediction_plot.png" width="650"></p>
</div>
</div>
<div class="section level2">
<h2 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<p>In this tutorial, we provided an overview of how to use the new
<strong><em>Univariate Predictive Analytics</em></strong> module with a
simulated data set. We showed that we can evaluate the predictive
performance of a variety of statistical models (ranging from linear
regression, classical time series models and machine learning
algorithms) via both point-based and probabilistic prediction metrics.
We also showed how we can use ensemble Bayesian Model Averaging to
combine the predictions of different models to obtain better predictions
compared to any single model. Finally, we showed how we can use these
model predictions to predict whether a process will go out of
control.</p>
<div class="section level3">
<h3 id="additional-features">Additional Features<a class="anchor" aria-label="anchor" href="#additional-features"></a>
</h3>
<p>The present tutorial did not cover all functionality of the module as
this would have been to extensive. In the future, we will add more
tutorials to explain the rest of the features. A few notable examples
are:</p>
<ul>
<li><p><em>Time Series Diagnostics</em>: The module also offers several
diagnostic tools such as Autocorrelation Function Plots - which enable
us to determine how related our dependent variable is with itself and
thus how suitable it is for time series models. One can also create
outlier tables if one is concerned with determining which specific
observation went out-of-control.</p></li>
<li><p><em>Feature Engineering:</em> Some of the models such as BART
cannot model autoregressive or seasonal components by default. Thus, we
added the ability to manually created lagged versions of the dependent
variable to enable iterative predictions and add automatic time-variable
features that can allow these models to accurately model
seasonality.</p></li>
</ul>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by JASP Team.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
