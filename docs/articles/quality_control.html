<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="jaspPredictiveAnalytics">
<title>Univariate Predictive Analytics - Simulated data set • jaspPredictiveAnalytics</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><link href="../deps/Roboto-0.4.2/font.css" rel="stylesheet">
<link href="../deps/Roboto_Slab-0.4.2/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Univariate Predictive Analytics - Simulated data set">
<meta property="og:description" content="jaspPredictiveAnalytics">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">jaspPredictiveAnalytics</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/getting_started.html">Get Started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/tutorials.html">Tutorials</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/background.html">Background</a>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://jasp-stats.org/">JASP</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Univariate Predictive Analytics - Simulated data set</h1>
                        <h4 data-toc-skip class="author">Fridtjof
Petersen</h4>
            
      
      
      <div class="d-none name"><code>quality_control.Rmd</code></div>
    </div>

    
    
<p>The full functionality of the module is quite extensive but can
mainly be divided into 3 steps:</p>
<ol style="list-style-type: decimal">
<li>Determining the control bounds</li>
<li>Selection and validation of candidate models</li>
<li>Selecting the right model for future prediction</li>
</ol>
<p>The basic idea is to first set the control limits, select various
possible models and test their predictive accuracy on past data and
finally use these results to select the final model that we want to use
to predict the future. By using past performance as a proxy for future
performance we should be able to select a better predicting model.</p>
<p>We will demonstrate the functionality on a simulated data set that is
supposed to represent a real world production process where our goal is
to make sure that our process stays in control and intervene earlier if
the process is predicting that it will go out of control in the future.
Additionally, we also have some measures from arbitrary sensors at the
start of the cycle that are slightly related to the process. Since we
want to predict into the future, we cannot use the measures at the end
of the production cycle where each product is measured for its size.</p>
<p>The values of the process are primarily influenced by heat which
depends on whether the machine is turned over over night or when it is
turned off for maintenance. It runs everyday from 08:00 until 19:00 and
produces 60 pieces per minute.</p>
<p>Since one production cycle for one piece takes approximately 100 data
points from start to finish, our goal is to predict 100 data points into
the future. That way we could intervene if the results of the next cycle
were predicted to go out of control. Because if we would only notice at
the end of the production cycle we would need to discard all the data
from the previous cycle.</p>
<div class="section level2">
<h2 id="determining-the-control-bounds">Determining the control bounds<a class="anchor" aria-label="anchor" href="#determining-the-control-bounds"></a>
</h2>
<div class="section level3">
<h3 id="selecting-necessary-variables">Selecting necessary variables<a class="anchor" aria-label="anchor" href="#selecting-necessary-variables"></a>
</h3>
<p>As a first step we need to load the data into JASP which is shown in
the following image:</p>
<div class="float">
<img src="img/univar_tutorial_sim/add_variables.gif" alt="Figure 1: Selecting necessary variables"><div class="figcaption">Figure 1: Selecting necessary variables</div>
</div>
<p>But what do of these different options mean? Our <strong>Dependent
Variable <em>y</em></strong> is the continuous production process we
want to monitor and predict over time. The <strong>Time</strong>
variable <strong><em>time</em></strong> contains the time stamps that
must be supplied. Supplying <strong>Covariates</strong> and
<strong>Factors</strong> is optional since they are not necessary for
the classical time series models as these only rely on time. In our case
we do have them available and can include them to improve predictive
accuracy. We use the sensor readings from the beginning of the time
series as well as temperature reading. Additionally, we have a two
variables called <strong><em>day_new_time_since</em></strong> and
<strong><em>day_new_n_since</em></strong> that indicate how many pieces
have been produced since and how much time has passed since the start of
that day. They are useful because the time the machine has been on
directly related to its temperature. Lastly, the <strong>Include in
Training</strong> variable indicates whether a time point should be used
for training and verifying the model (set to 1) or whether it should be
used to predict the future to make decisions. While the observations of
the dependent variable are of course not available for these data points
as we want to predict them, it is needed to supply covariates and
factors if we included them to build the model.</p>
</div>
<div class="section level3">
<h3 id="control-plot-and-control-limits">Control plot and control limits<a class="anchor" aria-label="anchor" href="#control-plot-and-control-limits"></a>
</h3>
<p>After the variables have been supplied, we automatically get a basic
control plot in the result section of JASP:</p>
<div class="float">
<img src="img/univar_tutorial_sim/control_plot_no_adj.JPG" width="659" alt="Figure 2: Default control plot based on 2 standard deviations"><div class="figcaption">Figure 2: Default control plot based on 2
standard deviations</div>
</div>
<p>The red dashed lines indicate the control bounds that we don’t want
to cross. As a default the limit is set in such a way that all the data
that is more than 2 standard deviations away from the mean is flagged as
out-of-control.</p>
<p>From mere visual inspection alone we can already see two things:
There seems to be a pattern in the process where it abruptly starts high
and slowly declines towards the middle. (wording). As mentioned
previously, this directly relates to the effect of the temperature:
Since we measure the process over several days, the machine cools down
at night when turned off. A lower temperature causes the produced pieces
to be larger. While the machine is on continuously, the temperature
rises to a certain degree until the size of each piece is somewhat
stable.</p>
<p>As a second observation the process seems to go out-of-control at the
end of the time series quite rapidly. The hypothetical explanation is
that a part of the machine broke inside the machine and subsequently
heats up the machine much more drastically which explains the process
going out-of-control. Since it is undesirable to let the process
go-out-of control, we will test in this tutorial whether we can predict
whether the process goes out-of-control!</p>
<p>But let us first adjust the control limits. In production processes
we often know how large/small a produced object must be so be don’t have
to set the control bounds based on data. For our example we will shift
the control limits a bit up as only the last part of the time series
clearly goes out-of-control. We can do this under the <em>Time Series
Descriptives</em> section:</p>
<div class="float">
<img src="img/univar_tutorial_sim/select_bounds.gif" width="455" alt="Figure 3: Manually selecting custom control bounds"><div class="figcaption">Figure 3: Manually selecting custom control
bounds</div>
</div>
<p>Under the <em>Error Bound Selection</em> option we can select
<em>Manual Bounds</em> and then manually input the upper/lower limit of
8.2/7.15 which changes the plot into the following:</p>
<div class="float">
<img src="img/univar_tutorial_sim/adjusted_control_plot.png" width="514" alt="Figure 4: Adjusted control plot based on manually set limits"><div class="figcaption">Figure 4: Adjusted control plot based on
manually set limits</div>
</div>
<p>Now we have set our control limits and the next step is to select the
.</p>
</div>
</div>
<div class="section level2">
<h2 id="selection-and-validation-of-candidate-models">Selection and validation of candidate models<a class="anchor" aria-label="anchor" href="#selection-and-validation-of-candidate-models"></a>
</h2>
<div class="section level3">
<h3 id="choosing-evaluation-plan">Choosing Evaluation plan<a class="anchor" aria-label="anchor" href="#choosing-evaluation-plan"></a>
</h3>
<p>As mentioned previously, we want to choose the model predicts the
future the best. One method for this is cross validation where the
exisiting data is split into <strong>training</strong> and
<strong>test</strong> data set. The training set is used to train the
statistical model which then make predictions about the test set that
are compared to the real observation.</p>
<p>As the goal is to evaluate the predictive performance on unseen
future observation, normal cross validation that randomly selects
training and test data is not appropriate. Instead, the data is split in
such a way that the training data is always temporally before the test
data. In our example we want to train the models on some amount of past
data and then use thenext 100 data points as test data as this
constitutes one production cycle. Afterwards we shift the training
window ahead by a certain amount of data points, retrain our models and
then predict the next 100 data points. This is called rolling window
cross validation or historical historical forecast evaluation.</p>
<p>The settings for this procedure can be found under the “Forecast
Evaluation” section in JASP (see image). We will evaluate whether we can
predict that the process goes out-of-control in the last section.</p>
<div class="float">
<img src="img/univar_tutorial_sim/forecast_eval_plan.JPG" style="float: right" width="246" alt="Figure 5: Creating evaluation plan"><div class="figcaption">Figure 5: Creating evaluation plan</div>
</div>
<p>The <em>Training window</em> option refers to how many past data
points our model is trained on. Since we experience an underlying shift
during the last day of the process and it goes out of control, we will
only train the models on the past 75 data points. In practice it might
be unknownon how much data each model should be trained on and different
options should be considered. If the underlying data generating process
stays constant over time, the models are ideally trained on all of the
past - his can be achieved by checking the option for <em>Cumulative
Training.</em> If the process changes over time or predictive
performance deteriorates the underlying process might warrant shorter
training windows.</p>
<p>The <em>Prediction window</em> option determines how many data points
should be predicted into the future. This is ideally chosen in such a
way that it corresponds to the prediction horizon one is interested in
predicting (as different models might perform better in short and other
in long-term forecasts). In our example this corresponds to a prediction
horizon of 100 as this represents one full cycle.</p>
<p>The next option called <em>Skip between training slices</em>
indicates how many data points are shifted ahead for each prediction
slice.</p>
<p>The <em>Maximum nr. of slices option</em> determines how many folds
are created. Increasing this number leads to less prediction slices or
folds that are computed but might increase computation time. Especially
if computational time is scarce since we want to make a timely
prediction about the future, we might want to select a smaller number.
In our case we selected 6 as we want to only predict the last part of
the process where the data goes out-of-control.</p>
<p>When we check the <em>Show evaluation</em> option we can also see the
evaluation plan visualized to check which data the model will evaluate
(see Figure 6). Here we see that the blue colored data is the training
period that the models are trained on whereas the red period will be
predicted and compared with the actual observations.</p>
<div class="float">
<img src="img/univar_tutorial_sim/evaluation_plan.png" alt="Figure 6: Plotted forecast evaluation plan"><div class="figcaption">Figure 6: Plotted forecast evaluation plan</div>
</div>
</div>
<div class="section level3">
<h3 id="selecting-candidate-models">Selecting candidate models<a class="anchor" aria-label="anchor" href="#selecting-candidate-models"></a>
</h3>
<p>Now that we have a plan to evaluate the data, we can select the
models that we want to use to predict the future. We can find the option
for selecting different models under the Forecast Evaluation section
under Model Choice. On the left side of the selection option, all
available models are displayed while the right side indicates the models
we have chosen for prediction.</p>
<p>A little explanation on the naming convention for all models: The
model family is indicated as the first part of the model name before the
hyphen “-” and the text afterwards indicates the model specification.
For more a more in-depth explanation of all models and further resources
on them see the <a href="background.html#models">Background</a>
section.</p>
<div class="float">
<img src="img/univar_tutorial_sim/model_choices.JPG" width="455" alt="Figure 7: Displays all available prediction models"><div class="figcaption">Figure 7: Displays all available prediction
models</div>
</div>
<p>For example in our selected models, the <strong>prophet</strong>
model is a Bayesian time series model that automatically models
appropriate seasonality via fourier orders, trend and change points. As
it is a time series model, it predicts future observation of our
dependent variable based on past values. If we would want to add
covariates, we would select the <strong>prohpet - regression</strong>
model. Our second model denoted as <strong>bart - regression</strong> is
a Bayesian Additive Regression Tree model. It combines multiple decision
trees into a large ensemble model and is able to model high-dimensional
non-linear relationships. As it is not a time series by default, we need
to supply it at least some covariates which we do in our selection in
this example. Additionally, it is also possible to supply it with our
lagged dependent variable. If we enable this option in the
<strong>Feature Engineering</strong> section, internally a new column is
created which contains our dependent variable but at a previous time
point. That way even models that are not time series models can model
the relationship of a variable with itself over time. When the model
then predicts future data points in our test set, the predicted value
for time point <strong>t</strong> is then iteratively used as a input
covariate for the prediction of time point <strong>t+1.</strong> Note
that this takes a long time for the BART model as the model generally
takes a long time to compute.</p>
<p>After we have selected which models we would like to use for
prediction, the models are being trained and evaluated based on the
evaluation plan we specified earlier. After the computations are
finished, we get a table with the corresponding evaluation metrics and
are also able to plot our predictions.</p>
</div>
<div class="section level3">
<h3 id="choosing-an-appropriate-evaluation-metrics">Choosing an appropriate evaluation metrics<a class="anchor" aria-label="anchor" href="#choosing-an-appropriate-evaluation-metrics"></a>
</h3>
<p>One can evaluate the prediction accuracy of our models by computing
so called evaluation or forecast metrics that measure how well our
models predict the future by comparing the actual observations of the
test set with the predictions. The JASP module offers both probabilistic
and point-based prediction metrics: Instead of relying on the point or
mean prediction of a model, probabilistic forecast metrics compute how
much the observed value differs from the whole predictive distribution.
That way we are able to take into account the uncertainty that is
associated with our predictive distribution. Ideally our distribution is
very sharp as this indicates that we are very certain in our
prediction.</p>
<p>For the current example we focus on the mean absolute error (MAE),
the Continuous Ranked Probability Score (CRPS) and the R-squared value.
As the name suggests the MAE is calculated by calculating how each
observations differs from the forecast in absolute terms (i.e. removing
the minus of negative numbers) and averaging it across all predicted
observations. The CRPS on the other generalizes the MAE as it computes
it for the whole predictive distribution instead of a single forecast
for each observation. The R-squared value on the other hand computes how
much variance is explained by the forecast. The other forecast metrics
are explained in detail <a href="general_documentation.html#forecast-evaluation-metric-table">here</a>.</p>
<p>Now let’s look at the table with forecast evaluation metrics</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by JASP Team.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
