[{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/background.html","id":"general-documentation","dir":"Articles","previous_headings":"","what":"General Documentation","title":"Background","text":"document provides exact information info button  JASP completeness searchability Univariate Predictive Analytics analysis.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/background.html","id":"probalistic-prediction","dir":"Articles","previous_headings":"","what":"Probalistic Prediction","title":"Background","text":"work progress","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/background.html","id":"models","dir":"Articles","previous_headings":"","what":"Models","title":"Background","text":"work progress","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/background.html","id":"ensemble-bayesian-model-averaging","dir":"Articles","previous_headings":"","what":"Ensemble Bayesian Model Averaging","title":"Background","text":"work progress","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"predictive-analytics","dir":"Articles","previous_headings":"","what":"Predictive Analytics","title":"Background","text":"module allows user perform probalistic time series forecasting.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"assumptions","dir":"Articles","previous_headings":"Predictive Analytics","what":"Assumptions","title":"Background","text":"missing covariates factors","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"assignment-box","dir":"Articles","previous_headings":"Predictive Analytics > Input","what":"Assignment Box","title":"Background","text":"Dependent Variable: Time series variable predicted (needed) Time: Time variable corresponds time stamp observation. Can following formats: [‘YYYY-MM-DD’, ‘YYYY/MM/DD’, ‘YYYY-MM-DD HH:MM:SS’, ‘YYYY/MM/DD HH:MM:SS’] (needed)“) Covariates: Covariates used prediction model Factors: Factors used prediction model Training Indicator: Logical variable (0 1) indicating cases used training verifying models (= 1) cases predicted (= 0). variable necessary making predictions covariates factors supplied","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"error-bound-selection-method","dir":"Articles","previous_headings":"Predictive Analytics > Input > Time Series Descriptive","what":"Error Bound Selection Method","title":"Background","text":"Data Based: Bounds automatically selected based mean many standard deviations data away mean. σ threshold: Indicated number standard deviations used calculate bound. example, set 2, data 2 standard deviations away flagged --control Trimmed mean: mean determines bounds calculated discarding upper lower xx quantile sample. Custom period: mean calculated based indicated period. Useful specific period available process known control. Manual bounds: Bounds manually set Upper/Lower bound: Determines separately upper lower bound .","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"control-plots","dir":"Articles","previous_headings":"Predictive Analytics > Input > Time Series Descriptive","what":"Control Plots","title":"Background","text":"Spread points equally: Ignores timestamp data point plots sequentially distance . Points/Line/: Displays data points either points, line time. Y-Axis Limits: Plot either shows data available focuses control bounds might cut outliers. Enable grid: Shows grid control chart Custom plot focus: Adds separate plot shows data indicated time period. --bound percent threshold: Sets percent data needs --bound trigger warning.","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"tables","dir":"Articles","previous_headings":"Predictive Analytics > Input > Diagnostics","what":"Tables","title":"Background","text":"Summary statistics: Table shows summary statistics time series variable aggregated whether data control. Transpose table: Transposes table data points shown columns variables shown rows. Custom table focus: shows outliers indicated time period.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"plots","dir":"Articles","previous_headings":"Predictive Analytics > Input > Diagnostics","what":"Plots","title":"Background","text":"Lags: Maximum number lags shown. Partial autocorrelation function: Additionally shows partial autocorrelation function plot.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"feature-engineering","dir":"Articles","previous_headings":"Predictive Analytics > Input","what":"Feature Engineering","title":"Background","text":"Options create automatic time series features might improve prediction accuracy models. - Nr. lags: Number lags used create lagged time series variables features. --sample predictions, previous predictions used create lagged variables prevent data leakage. - Automatic time-based features: Creates multiple features based time variable. example, create column indicates specific month (1-12), day (1-31), hour (1-24), minute (1-60)etc. time stamp. useful models don’t model seasonality automatically. - Remove zero-variance variables: Removes variables zero variance. - Remove variables stronger correlated : Removes variables strongly correlated . threshold set user.","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"evaluation-plan","dir":"Articles","previous_headings":"Predictive Analytics > Input > Forecast Evaluation","what":"Evaluation Plan","title":"Background","text":"section allows evaluating selected models performs best terms predictive performance available data. helpful determining model used future predictions. goal evaluate predictive performance unseen future observation, normal cross validation randomly selects training test data appropriate. Instead, data split way training data always test data. example model trained first 100 observations predictive performance evaluated next 10 observations. Afterwards data shifted ahead certain amount observations model process repeated multiple times. called rolling window cross validation. predictive performance evaluated deterministic probabilistic metrics averaged across slices. - Training window: Indicates number observations used training model. - Prediction window: Indicates number observations used assess predictive performance model. - Select slices : Indicates whether slices selected start end data. - Maximum nr. slices: Indicates maximum number slices used evaluation. actual number slices might lower data long enough. - Cumulative training: Indicates training window grows cumulative slice . Increases time needed evaluation models trained longer longer time periods. - Show evaluation plan: Plots evaluation plan used model evaluation. - Spread points equally: Ignores timestamp data point plots sequentially distance . - Max slices shown: Maximum number slices shown plot. Actual number can higher.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"model-choice","dir":"Articles","previous_headings":"Predictive Analytics > Input > Forecast Evaluation","what":"Model Choice","title":"Background","text":"Select models used prediction evaluated. model family indicated first part model name hyphen “-” indicates model specification afterwards. example model “linear regression - regression + lag” normal linear Bayesian regression model uses covariates factors input well lagged variables created “Feature Engineering” section. specific model families model specifications explained .","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"model-families","dir":"Articles","previous_headings":"","what":"Background","title":"Background","text":"linear regression: Basic Bayesian linear regression model uses spike slab prior variable selection. bsts: Bayesian structural time series model decomposes time series different hidden components. Models hidden state either linear trend auto-regressive process. Can also include regressors via spike slab regression additional component. prophet: Bayesian time series model automatically models appropriate seasonality via fourier orders, trend change points. Can also include regressors additional component. xgboost: Gradient boosting model uses tree-based model predict time series variable. automatically model seasonality. probalisitic. bart: Bayesian additive regression tree model similar “sum--tree” models regularises impact tree via prior. automatically model seasonality.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"model-specifications","dir":"Articles","previous_headings":"","what":"Background","title":"Background","text":"time: uses time variable input. Useful baseline model compare complex models. regression: uses covariates factors input well automatic time-based features enabled lag: Additionally uses lagged time series variables input.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"evaluation-metrics","dir":"Articles","previous_headings":"Predictive Analytics > Input > Forecast Evaluation","what":"Evaluation Metrics","title":"Background","text":"probabilistic metrics available models probabilistic assess prediction performance based whole predictive density taking account uncertainty around forecast. deterministic metrics available models take account mean forecast. metrics explained .","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"probabilistic-metrics","dir":"Articles","previous_headings":"","what":"Background","title":"Background","text":"Continuous ranked probability score (CRPS): Display CRPS. Dawid-Sebastiani score (DSS): Display DSS. Log score (LS): Display LS. Coverage: Display coverage. Bias: Display bias. Probability integral transform (PIT): Display PIT.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"probabilistic-metrics-1","dir":"Articles","previous_headings":"","what":"Background","title":"Background","text":"Mean absolute error (MAE): Display MAE. Root mean squared error (RMSE): Display RMSE. R-squared: Display R-squared. PIT Binned Density Plots: Select models PIT binned density plots shown.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"prediction-plots","dir":"Articles","previous_headings":"Predictive Analytics > Input","what":"Prediction Plots","title":"Background","text":"Models plot: Select models --sample predictions shown evaluation metrics calculated .","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"ensemble-bayesian-model-averaging","dir":"Articles","previous_headings":"Predictive Analytics > Input","what":"Ensemble Bayesian Model Averaging","title":"Background","text":"Ensemble Bayesian model averaging (eBMA) performs model averaging across --sample predictions several models. weights model estimated based predictive performance model computed slice. goal quantify uncertainty model selection improve predictive performance. weights previous slice used adjust predictions current slice - evaluation metrics calculated adjusted predictions. Expectation-Maximisation (EM): Uses expectation-maximization algorithm estimate weights model. default faster. Sets model weights zero Gibbs sampling. Gibbs: Performs Gibbs sampling estimate weights model. Slower produces full posterior distribution model. Sets fewer model weights zero EM. Show per slice: Instead averaging weights across slices, show weights slice.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"future-prediction","dir":"Articles","previous_headings":"Predictive Analytics > Input","what":"Future Prediction","title":"Background","text":"section includes functionality predict future time series. number data points used train model future predictions default set training window used forecast evaluation. Also allows warning message via reporting mode indicated probability threshold crossing control bounds exceeded. Model Selection: Choose model used future predictions. Prediction horizon: Choose number time points predicted future. Last xx data points: Specify exact number. data points: Use data points available train prediction model. Spread points equally: Ignores timestamp data point plots sequentially distance . --bound probability threshold: Set probability threshold crossing control bounds triggers warning message. example, set 20% warning message triggered 20th quantile predictive distribution outside control bounds.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"advanced-options","dir":"Articles","previous_headings":"Predictive Analytics > Input","what":"Advanced Options","title":"Background","text":"Parrallel model computation: Select whether models computed parallel. can speed computation time can also produce errors windows. Skip training slices: Selects time points training window moved forward training window. default, set prediction window.","code":""},{"path":[]},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"basic-control-plot","dir":"Articles","previous_headings":"Predictive Analytics > Output > Time Series Descriptives","what":"Basic Control Plot","title":"Background","text":"Displays time stamps (time points) x-axis time series variable y-axis. control limits displayed dashed horizontal lines. control limits based settings control bounds. Data points outside control bounds displayed red points/lines, whereas data points within control bounds displayed blue points/lines.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"basic-control-plot---focused","dir":"Articles","previous_headings":"Predictive Analytics > Output > Time Series Descriptives","what":"Basic Control Plot - Focused","title":"Background","text":"basic control plot displays subset data indicated user.","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"control-summary-table","dir":"Articles","previous_headings":"Predictive Analytics > Output > Diagnostics","what":"Control Summary Table","title":"Background","text":"Displays summary statistics time series divides depending whether data within outside control bounds. - Control Area: - : data points. - : Data points upper control limit. - : Data points lower control limit. - Inside: Data points within control bounds. - Mean: Mean data points. - SD: Standard deviation data points. - Minumum: Minimum value. - Maximum: Maximum value. - Valid: Number valid data points missing invalid. - Percent: Percentage valid data points fall category. - Average Deviation: Average distance corresponding control limit.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"outlier-table","dir":"Articles","previous_headings":"Predictive Analytics > Output > Diagnostics","what":"Outlier Table","title":"Background","text":"Table displays time stamp (time point) value data points outside control bounds. - Time: Time stamp (time point) data point. - Control Area: Whether data point control bounds. - Value: Value data point outside control bounds. - Deviation: Distance corresponding control limit.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"histogram","dir":"Articles","previous_headings":"Predictive Analytics > Output > Diagnostics","what":"Histogram","title":"Background","text":"Displays histogram counts y-axis values x-axis. Data points outside control bounds displayed red bars, whereas data points within control bounds displayed blue bars.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"autocorrelation-function-plot","dir":"Articles","previous_headings":"Predictive Analytics > Output > Diagnostics","what":"Autocorrelation Function Plot","title":"Background","text":"Displays strength autocorrelation y-axis lags x-axis dashed line indicates 95% confidence interval. higher autocorrelation, similar data points data points previous time point. autocorrelation plot used determine whether time series stationary .","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"partial-autocorrelation-plot","dir":"Articles","previous_headings":"Predictive Analytics > Output > Diagnostics","what":"Partial Autocorrelation Plot","title":"Background","text":"autocorrelation plot displays autocorrelation already explained autocorrelation smaller lag.","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"forecast-evaluation-plan","dir":"Articles","previous_headings":"Predictive Analytics > Output > Forecast Evaluation","what":"Forecast Evaluation Plan","title":"Background","text":"Plot displays forecasting performance models evaluated. x-axis displays time points y-axis displays number data points used train model forecast evaluation. blue line indicates training window red line indicates prediction window. plot split different rows row represents single training slice.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"forecast-evaluation-metric-table","dir":"Articles","previous_headings":"Predictive Analytics > Output > Forecast Evaluation","what":"Forecast Evaluation Metric Table","title":"Background","text":"metrics averaged across slices based --sample prediction. metrics : - Model: Name model. - Continuous ranked probability score (CRPS): Compares predicted cumulative density function (CDF) actual CDF. Generalisation mean absolute error (MAE) whole predictive density. lower CRPS, better model. - Dawid-Sebastiani score (DSS): Compares predictive density actual CDF mean variance. lower DSS, better model. - Log score (LS): Compares predictive density actual CDF measuring logarithmic difference two. lower LS, better model. - Coverage: Proportion observations within 95% prediction interval. higher coverage, better model. - Bias: Indicates whether predictions systematically high low. bias -1 predictions smaller actual value. bias 1 predictions larger actual value. closer bias 0, unbiased model. - Probability Integral Transform (PIT): Evaluates calibration model treating test observation random variable coming predictive distribution. PIT value 0.5 model perfectly calibrated. PIT value 0 model overconfident PIT value 1 model underconfident. closer PIT value 0.5, better model.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"pit-density-density-plot","dir":"Articles","previous_headings":"Predictive Analytics > Output > Forecast Evaluation","what":"PIT Density Density Plot","title":"Background","text":"Plots PIT values model averaged across slices. x-axis displays PIT values y-axis displays density. model perfectly calibrated density uniform across PIT values density 0.1 bar.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"prediction-plot","dir":"Articles","previous_headings":"Predictive Analytics > Output > Forecast Evaluation","what":"Prediction Plot","title":"Background","text":"Displays actual observations predictions selected models. x-axis displays time points y-axis displays values time series variable. different models displayed different colors corresponding model names displayed legend.","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"bma---model-weights","dir":"Articles","previous_headings":"Predictive Analytics > Output > Ensemble Bayesian Model Averaging","what":"BMA - Model Weights","title":"Background","text":"Displays model weights either averaged across slices slice seperately. - Model: Name model. - Weights: Corresponding toe model weight assigned eBMA. Higher weights indicates better predictive performance. Formally defined probability observations originated model.","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/general_documentation.html","id":"future-prediction-plot","dir":"Articles","previous_headings":"Predictive Analytics > Output > Future Prediction","what":"Future Prediction Plot","title":"Background","text":"Plots predictions unobserved future. black dashed line indicates start prediction. blue area indicates 95% prediction interval. x-axis displays time points y-axis displays values time series variable. red dashed line present indicates time point control bounds exceeded depending probability threshold.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/getting_started.html","id":"process-control","dir":"Articles","previous_headings":"","what":"Process Control","title":"Getting Started","text":"process control, usually process continuously generates data want monitor time make sure stays within desired boundaries time. example, health care setting might interested monitoring absenteeism rate employees order detect potential outbreak diseases (Woodall & Montgomery, 2014). industry context, might continuous production process interested monitoring dimension produced product ensure reliable enough sold. Univariate Predictive Analytics analysis offers several options determine control limits determine whether process --bound . control limits can either set manually based data selecting number standard deviations mean beyond data considered control. Alternatively, control limits can directly computed (subset) dataset investigated. Additionally, includes basic quality control chart, process control summary statistics outlier tables. Multivariate Binomial Control currently allows manually set control bounds several variables time. includes control charts exact amount --control data points well overall proportion time.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/getting_started.html","id":"probabilistic-time-series-prediction","dir":"Articles","previous_headings":"","what":"Probabilistic Time Series Prediction","title":"Getting Started","text":"interested monitoring whether process currently --control - bur rather whether going control future. can make use time series prediction. especially valuable production process multiple production steps: Detecting system produces faulty parts end production cycle increases loss parts current cycle might need discarded. Thus able predict whether process going go control can significantly save cost time enables act advance. Thus Univariate Predictive Analytics analysis includes several time series prediction models trained historical data make predictions future. available models range classical time series models (e.g. state space models, prophet) flexible machine learning models (Bayesian additive regression trees). Additionally, put emphasis probabilistic forecasting method (Gneiting & Katzfuss, 2014) allows us quantify uncertainty estimates - predict process go control also probability. Multivariate Binomial Control hand allows us predict overall proportion --control variables evolve time - whether reaches certain threshold.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/getting_started.html","id":"forecast-evaluation","dir":"Articles","previous_headings":"","what":"Forecast Evaluation","title":"Getting Started","text":"order make decisions based predictions need confidence accuracy. One common way evaluate forecast accuracy repeatedly subset available historical data training test data set. training set used train statistical model make predictions test set compared real observation. can compute various forecasting metrics quantify accuracy model help determining best model. Currently Univariate Predictive Analytics analysis offers forecasting evaluation. accuracy assessed forecast metrics based point predictions (R-squared mean absolute error) well probabilistic metrics take account whole predictive distribution (e.g. Logarithmic score Continuous Ranked Probability Score).","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/getting_started.html","id":"ensemble-bayesian-model-averaging","dir":"Articles","previous_headings":"","what":"Ensemble Bayesian Model Averaging","title":"Getting Started","text":"Instead relying single best model predict future, can also combine predictions different models. First, benefit takes account uncertainty model selection simply selecting single best model might produce overconfident results (Wagenmakers et al., 2022). Additionally, problem hand might complex represented well single model - combining different models might provide better predictuon model capture unique aspect problem hand (Sagi & Rokach, 2018). last point, reduces chances selecting wrong model user don’t manually pick one. Univariate Predictive Analytics analysis includes ensemble Bayesian model averaging method combine predictions different models(Raftery et al., 2005). model weighted according historical predictive accuracy (see previous section) predictions combined predict future. model weight can interpreted relative probability true model.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/getting_started.html","id":"reporting-mode","dir":"Articles","previous_headings":"","what":"Reporting Mode","title":"Getting Started","text":"novelty, module also adds new Reporting Mode functionality recently added JASP. way operators users can informed process reaches threshold predicted future. Especially JASP can now also connect frequently used databases, can integrate live production processes provide near real time feedback!","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"selecting-necessary-variables","dir":"Articles","previous_headings":"Determining the control bounds","what":"Selecting necessary variables","title":"Univariate Predictive Analytics - Simulated data set","text":"first step need load data JASP shown following image: different options mean? Dependent Variable y continuous production process want monitor predict. Time variable time contains time stamps must supplied. Supplying Covariates Factors optional since necessary classical time series models - needed complex models. case include sensor measurements associated dependent variable thus improve predictive accuracy. Additionally, two variables called day_new_time_since day_new_n_since indicate many pieces produced since much time passed since start day. useful time machine directly related temperature - affects size produced pieces. Lastly, Include Training variable indicates whether time point used training verifying model (set 1) whether used predict future (set 1). observations dependent variable course available data points want predict , needed supply covariates factors included models.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"control-plot-and-control-limits","dir":"Articles","previous_headings":"Determining the control bounds","what":"Control plot and control limits","title":"Univariate Predictive Analytics - Simulated data set","text":"variables supplied, automatically get basic control plot result section JASP: red dashed lines indicate control bounds don’t want cross. default option, control bounds set way data 2 standard deviations away mean flagged --control. mere visual inspection alone can already see two things: seems cyclic pattern process abruptly jumps size slowly becomes slower . mentioned previously, directly relates effect temperature: Since measure process several days, machine cools night turned explains sudden jump. lower temperature causes produced pieces larger. However machine turned continuously, temperature slowly rises certain maximum temperature size piece somewhat stable. second observation process seems go --control end time series quite rapidly. hypothetical explanation part machine broke inside machine subsequently heats machine much drastically explains process going --control rather quickly. Since undesirable let process go--control, test tutorial whether can predict whether process goes --control! Afterwards illustrate predict unseen future. let us first adjust control limits. production processes often know large/small produced object must don’t set control bounds based data standard deviations. example, increase control limits somewhat last part time series clearly goes --control. can Time Series Descriptives section: Error Bound Selection option can select Manual Bounds manually input upper/lower limit 8.2/7.15 changes plot following:","code":""},{"path":[]},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"choosing-evaluation-plan","dir":"Articles","previous_headings":"Selecting and evaluating the candidate models","what":"Choosing Evaluation plan","title":"Univariate Predictive Analytics - Simulated data set","text":"mentioned previously, want choose model predicts future best. One method called cross validation existing data randomly split training test data sets. training set used train statistical model make predictions test set compared real observation. comparing predictions true observations can test well model predicts unseen data generalises. goal evaluate predictive performance unseen future observation, normal cross validation randomly selects training test data appropriate. Instead, data split way training data always temporally test data. example want train models amount past data use next 100 data points test data constitutes one production cycle. Afterwards shift training window ahead certain amount data points, retrain models predict next 100 data points. called rolling window cross validation historical historical forecast evaluation. settings procedure can found “Forecast Evaluation” section zhe JASP module: Training window option refers many past data points model trained . Since experience underlying shift last day process goes control, train models past 75 data points. practice might unknown much data model trained different options considered. underlying data generating process stays constant time, models ideally trained past - can achieved checking option Cumulative Training. process changes (quickly) time - indicated worse predictive performance shorter training window might better choice. Prediction window option determines many data points predicted future. ideally chosen way corresponds prediction horizon one interested predicting (different models might perform better short long-term forecasts). example, corresponds prediction horizon 100 represents one full cycle. next option called Skip training slices indicates many data points shifted ahead prediction slice. Maximum nr. slices option determines many folds created. Increasing number leads less prediction slices folds computed might increase computation time. Especially computational time scarce since want make timely prediction future, might want select smaller number. case selected 6 want predict last part process data goes --control. check Show evaluation option can also see evaluation plan visualized check data model evaluate (see Figure 6). see blue colored data training period models trained - whereas red period predicted compared actual observations.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"selecting-candidate-models","dir":"Articles","previous_headings":"Selecting and evaluating the candidate models","what":"Selecting candidate models","title":"Univariate Predictive Analytics - Simulated data set","text":"Now plan evaluate data, can select models want use predict future. can find option selecting different models Forecast Evaluation section Model Choice. left side selection option, available models displayed right side indicates models chosen prediction. little explanation naming convention models: model family indicated first part model name hyphen “-” text afterwards indicates model specification. -depth explanation models resources see Background section. example selected models, prophet model Bayesian time series model automatically models appropriate seasonality via fourier orders, trends change points. time series model, predicts future observation dependent variable based past values. want add covariates, select prohpet - regression model. second model denoted bart - regression Bayesian Additive Regression Tree model. combines multiple decision trees large ensemble model able model high-dimensional non-linear relationships. time series default, need supply least covariates case sensors. Additionally, also possible supply lagged dependent variable. enable option Feature Engineering section, internally new column created contains dependent variable previous time point. way even models time series models can model relationship variable time. model predicts future data points test set, predicted value time point t iteratively used input covariate prediction time point t+1. Note takes long time BART model model computationally complex. selected models like use prediction, models trained evaluated based evaluation plan specified earlier. computations finished, get table corresponding evaluation metrics also able plot predictions.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"choosing-an-appropriate-evaluation-metrics","dir":"Articles","previous_headings":"Selecting and evaluating the candidate models","what":"Choosing an appropriate evaluation metrics","title":"Univariate Predictive Analytics - Simulated data set","text":"One can evaluate prediction accuracy models computing called evaluation forecast metrics measure well models predict future comparing actual observations test set predictions. JASP module offers probabilistic point-based prediction metrics: Instead relying point mean prediction model, probabilistic forecast metrics compute much observed value differs whole predictive distribution. way able take account uncertainty associated predictive distribution. Ideally, distribution sharp indicates certain prediction. current example focus mean absolute error (MAE), Continuous Ranked Probability Score (CRPS) R-squared value. name suggests MAE calculated calculating observations differs forecast absolute terms (.e. removing minus negative numbers) averaging across predicted observations. CRPS generalizes MAE computes whole predictive distribution instead single forecast value observation. R-squared value hand computes much variance explained forecast. forecast metrics explained detail . general, forecast evaluation metrics can selected section Forecast Evaluation -> Evaluation Metrics:","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"forecast-evaluation-metric-table","dir":"Articles","previous_headings":"Selecting and evaluating the candidate models > Choosing an appropriate evaluation metrics","what":"Forecast Evaluation Metric Table","title":"Univariate Predictive Analytics - Simulated data set","text":"Now let’s look table forecast evaluation metrics. first row indicates corresponding model name. rows indicate values corresponding evaluation metric. values averaged across evaluation slices set previously. Lower MAE CRPS values indicate prediction performs better higher values \\(R^2\\) indicate better prediction. MAE CRPS dependent scale data (.e. multiply observations 10, MAE values also increase 10), \\(R^2\\) values scale independent value \\(R^2 = 1\\) indicate observations predicted perfectly model value \\(R^2 =0\\) predictions fail completely. comparing values model can determine candidate models predicts best. comparing prophet BART model , can see prediction prophet model generally predict better CRPS MAE values lower. hand \\(R^2\\) value BART model much higher can explained fact model flexible. Thus explains variance observed data, extra flexibility makes predictions wrong average - measured MAE CRPS. become obvious observe plots data.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"forecast-plots","dir":"Articles","previous_headings":"Selecting and evaluating the candidate models > Choosing an appropriate evaluation metrics","what":"Forecast Plots","title":"Univariate Predictive Analytics - Simulated data set","text":"can also visualize predictions model real observations. can select models want view section Prediction Plots.  Afterwards corresponding prediction plot shown JASP output. shown legend bottom, different line colors represent different model types. real observed data shown purple, prophet model green BART model yellow. real data depicts training window used trail models well prediction horizon used calculate evaluation metrics. two dashed red lines depict control limits can verify whether models accurately predict data goes control. evaluation slice shown separate plot.  general, can see data predicted quite well models. process seems go really control last two slices/around data point 2500 visible overall control plot Figure 4 beginning tutorial. prophet model seems better predicting overall trend data, BART model predicts fluctuation data better pick well downward trend. makes sense prophet model designed predict trend seasonality time series data, BART model better predicting relationships covariates explicitly model relationship process . also aligns results evaluation metric table shown previous step: prophet model also lower error measured MAE CRPS BART model explained variance real observations. general can conclude decent job predicting process goes control.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"selecting-the-best-model-for-future-prediction","dir":"Articles","previous_headings":"","what":"Selecting the best model for future prediction","title":"Univariate Predictive Analytics - Simulated data set","text":"evaluated predictive performance models past data, ready select model candidate pool want use predict future. metric table prediction plot, observed prophet model better job predicting overall trend process, BART model better predicting variance data. ideal world, single model good trend forecasting complex covariate relationships. feasible computational statistical standpoint however, can instead combine predictions different models. way one doesn’t choose manually different models - might benefit applied user module - one can potentially improve predictive accuracy compared choosing single model. One method combine different model prediction - implemented module - called forecasts ensemble Bayesian model averaging. discussed briefly extensively .","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"ensemble-bayesian-model-averaging","dir":"Articles","previous_headings":"Selecting the best model for future prediction","what":"Ensemble Bayesian Model Averaging","title":"Univariate Predictive Analytics - Simulated data set","text":"general, ensemble Bayesian model averaging (eBMA) consists two steps: First, model weights assigned models based past predictive performance. weights also called posterior model probabilities can interpreted relative likelihood true observations originated corresponding evaluated model - compared considered models. Secondly, weights used weight individual model predictions future data finally sum combine single ensemble model. present JASP model, weights estimated separately evaluation slice, used adjust predictions next slice. compute evaluation metrics eBMA predictions can determine predictive benefit compared single model. Finally, use model weights last evaluation slice adjust --sample predictions future. order use ensemble Bayesian model averaging (eBMA) JASP, simply need check checkbox Perform eBMA section Ensemble Bayesian Model Averaging. Prerequisite performed forecasting evaluation least two models previously. Method option determines model weights determined default set faster Expectation-maximization algorithm. Evaluation Method determines predictive benefit eBMA evaluated. Additionally, can also show model weights averaged across evaluation slices - per individual slice want know specifically weights used adjust prediction  result obtain table model weights adjusted evaluation metric table includes eBMA model. Across slices prophet model receives slightly slightly higher model weight indicates likely best model. line evaluation table prophet model outperformed BART model slightly well terms MAE CRPS. looking evaluation metrics can see eBMA predictions decreases CRPS values ~17 percent MAE values ~15 percent compared prophet model. looking \\(R^2\\) values, eBMA approach increased metric 12.5 percent. based metrics can argue combining predictions different model via eBMA indeed improved prediction accuracy substantially.   15- 20 percent actually look like? example data set hypothetically want predict whether process goes control. increase prediction enough make better decisions whether turn machine intervene different way ahead time? can look prediction plots discussed earlier. can now also visualize eBMA predictions. example investigate two prediction slices. cases predictions prophet model (blue) bit low predictions BART model bit high (green). eBMA predictions however better overall somewhat closer real observations displayed purple.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"predicting-the-future","dir":"Articles","previous_headings":"Selecting the best model for future prediction","what":"Predicting the Future","title":"Univariate Predictive Analytics - Simulated data set","text":"far set control limits, evaluated models explored whether eBMA approach can help us making better predictions. Now finally time predict actual future. Let us pretend new day started tested whether module principle predict whether process goes --control. time fixed production machine replaced parts hopefully stop machine going control . Let us also pretend database connected module updated new data already arrived. process gone control corresponding control plot looks like :  predict future, need open section titled Future Prediction. first option let’s us select model want use predict future. simply use ensemble Bayesian Model Averaging weights selected models automatically us - previously shown can provide predictive benefit single model. model weights taken recent training adjust future predictions. values prediction horizon (far want predict future) - well training window (much past data want use training) default set values selected model evaluation. Alternatively, also set custom values train models past models. Future prediction plotcheckbox automatically enabled soon model chosen prediction.  Now lastly can look prediction plot appears soon computations finished. horizontal red dashed lines represent control bounds. data depicted black vertical dashed line represent real data data afterwards depicts predicted future. blue area depicts 95 percent credible interval can interpreted following way: Given past data predictions different models, eBMA method predicts unknown future observations fall within interval probability 95 percent. neither black line (mean prediction) credible interval fall outside control limits, can assume process go --control within next 100 data points.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Univariate Predictive Analytics - Simulated data set","text":"tutorial, provided overview use new Univariate Predictive Analytics module simulated data set. showed can evaluate predictive performance variety statistical models (ranging linear regression, classical time series models machine learning algorithms) based point-based probabilistic prediction metrics. also showed can use ensemble Bayesian Model Averaging combine predictions different models obtain better predictions compared single model. Finally, showed can use model predictions predict whether process go control.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/quality_control.html","id":"additional-features","dir":"Articles","previous_headings":"Summary","what":"Additional Features","title":"Univariate Predictive Analytics - Simulated data set","text":"present tutorial cover functionality module extensive. future, add tutorials explain rest features. notable examples : Time Series Diagnostics: module also offers several diagnostic tools Autocorrelation Function Plots - enable us determine related dependent variable thus suitable time series models. One can also create outlier tables one concerned determining specific observation went --control. Feature Engineering: models BART model autoregressive seasonal components default. Thus, added ability manually created lagged versions dependent variable enable iterative predictions add automatic time-variable features can allow models accurately model seasonality.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/tutorials.html","id":"univariate-predictive-analytics","dir":"Articles","previous_headings":"","what":"Univariate Predictive Analytics","title":"Tutorials","text":"Simulated data set   Shows main capabilities module simulated manufacturing process influenced temperature machine, wear machine parts random error.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/articles/tutorials.html","id":"multivariate-binomial-control","dir":"Articles","previous_headings":"","what":"Multivariate Binomial Control","title":"Tutorials","text":"WORK PROGRESS","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"JASP Team. Maintainer.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Team J (2023). jaspPredictiveAnalytics: Probalistic time series forecasting focus quality control predictions.. R package version 0.1.0, https://petersen-f.github.io/jaspPredictiveAnalytics/.","code":"@Manual{,   title = {jaspPredictiveAnalytics: Probalistic time series forecasting with a focus on quality control predictions.},   author = {JASP Team},   year = {2023},   note = {R package version 0.1.0},   url = {https://petersen-f.github.io/jaspPredictiveAnalytics/}, }"},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/index.html","id":"jasppredictiveanalytics-","dir":"","previous_headings":"","what":"Probalistic time series forecasting with a focus on quality control predictions. ","title":"Probalistic time series forecasting with a focus on quality control predictions. ","text":"Predictive Analytics module JASP adds extensive time series prediction methods JASP. combines classical time series methods machine learning models quality control concepts. Quality control techniques monitor whether certain process remains within predefined boundaries. way one can intervene boundaries crossed ensure product service desired quality. Time series forecasting allows us predict process develop future. way can predict ahead time whether process goes beyond boundaries. prediction models probabilistic, can predict process go control future also quantify uncertainty process go --control future.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting started","title":"Probalistic time series forecasting with a focus on quality control predictions. ","text":"get general overview package read Getting Started article. specific instructions use modules see tutorials. get -depth understanding functionality recommend reading background articles.","code":""},{"path":"https://petersen-f.github.io/jaspPredictiveAnalytics/index.html","id":"ressources","dir":"","previous_headings":"","what":"Ressources","title":"Probalistic time series forecasting with a focus on quality control predictions. ","text":"Read documentation (page) Open issue (Github issues report bugs request features) Ask question (Forum) Email maintainer","code":""}]
